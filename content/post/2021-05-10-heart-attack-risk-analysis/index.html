---
title: Heart attack risk analysis
author: ''
date: '2021-05-10'
slug: heart-attack-risk-analysis
categories: []
tags: []
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<pre class="r"><code>library(tidyverse)
library(readr)
library(broom)
theme_set(theme_minimal())

heart &lt;- read_csv(&quot;heart.csv&quot;)</code></pre>
<p>The dataset can be found on <a href="https://www.kaggle.com/rashikrahmanpritom/heart-attack-analysis-prediction-dataset">kaggle</a></p>
<pre class="r"><code>glimpse(heart)</code></pre>
<pre><code>## Rows: 303
## Columns: 14
## $ age      &lt;dbl&gt; 63, 37, 41, 56, 57, 57, 56, 44, 52, 57, 54, 48, 49, 64, 58, 5~
## $ sex      &lt;dbl&gt; 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1~
## $ cp       &lt;dbl&gt; 3, 2, 1, 1, 0, 0, 1, 1, 2, 2, 0, 2, 1, 3, 3, 2, 2, 3, 0, 3, 0~
## $ trtbps   &lt;dbl&gt; 145, 130, 130, 120, 120, 140, 140, 120, 172, 150, 140, 130, 1~
## $ chol     &lt;dbl&gt; 233, 250, 204, 236, 354, 192, 294, 263, 199, 168, 239, 275, 2~
## $ fbs      &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0~
## $ restecg  &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1~
## $ thalachh &lt;dbl&gt; 150, 187, 172, 178, 163, 148, 153, 173, 162, 174, 160, 139, 1~
## $ exng     &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0~
## $ oldpeak  &lt;dbl&gt; 2.3, 3.5, 1.4, 0.8, 0.6, 0.4, 1.3, 0.0, 0.5, 1.6, 1.2, 0.2, 0~
## $ slp      &lt;dbl&gt; 0, 0, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 0, 2, 2, 1~
## $ caa      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0~
## $ thall    &lt;dbl&gt; 1, 2, 2, 2, 2, 1, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3~
## $ output   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~</code></pre>
<p>N = 303</p>
<div id="explanation-of-features" class="section level1">
<h1>Explanation of features</h1>
<ol style="list-style-type: decimal">
<li>age - age in years</li>
<li>sex - sex (1 = male; 0 = female)</li>
<li>cp - chest pain type (1 = typical angina; 2 = atypical angina; 3 = non-anginal pain; 0 = asymptomatic)</li>
<li>trestbps - resting blood pressure (in mm Hg on admission to the hospital)</li>
<li>chol - serum cholestoral in mg/dl</li>
<li>fbs - fasting blood sugar &gt; 120 mg/dl (1 = true; 0 = false)</li>
<li>restecg - resting electrocardiographic results (1 = normal; 2 = having ST-T wave abnormality; 0 = hypertrophy)</li>
<li>thalach - maximum heart rate achieved</li>
<li>exang - exercise induced angina (1 = yes; 0 = no)</li>
<li>oldpeak - ST depression induced by exercise relative to rest</li>
<li>slope - the slope of the peak exercise ST segment (2 = upsloping; 1 = flat; 0 = downsloping)</li>
<li>ca - number of major vessels (0-3) colored by flourosopy</li>
<li>thal - 2 = normal; 1 = fixed defect; 3 = reversable defect</li>
<li>output - diagnosis of heart disease</li>
</ol>
<p>It is my understanding that the ST is a specific interval of time during an electrocardiogram stress test.</p>
</div>
<div id="exploring-the-dataset" class="section level1">
<h1>Exploring the dataset</h1>
<pre class="r"><code>heart %&gt;% 
  count(output)</code></pre>
<pre><code>## # A tibble: 2 x 2
##   output     n
## *  &lt;dbl&gt; &lt;int&gt;
## 1      0   138
## 2      1   165</code></pre>
<p>The dataset represents 54.46% of people who suffer with heart disease.</p>
<div id="covariance-matrix" class="section level2">
<h2>Covariance matrix</h2>
<pre class="r"><code>g &lt;- cor(heart)

corrplot::corrplot(g)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>The covariance matrix above shows heart disease to be positively correlated with chest pain, the maximum heart rate achieved, and the incline type of exercise during the testing segment; and negatively correlated with exercise induced angina, and ST depression induced by exercise. I expect any machine learning model to take these correlations into account.</p>
</div>
<div id="age-distribution" class="section level2">
<h2>Age distribution</h2>
<pre class="r"><code>heart %&gt;% 
  ggplot(aes(age)) +
  geom_histogram(alpha = 0.5, fill = &quot;dodgerblue&quot;,binwidth = 5)+
  geom_vline(xintercept = mean(heart$age),lty = 2)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>heart %&gt;% 
  ggplot(aes(age))+
  geom_density()+
  geom_vline(xintercept = mean(heart$age),lty = 2)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>The observations within the dataset tends to skew older ages.</p>
</div>
<div id="sex-distribution" class="section level2">
<h2>Sex distribution</h2>
<pre class="r"><code>heart %&gt;% 
  count(sex)</code></pre>
<pre><code>## # A tibble: 2 x 2
##     sex     n
## * &lt;dbl&gt; &lt;int&gt;
## 1     0    96
## 2     1   207</code></pre>
<p>There are ~twice the amount of men featured in the dataset than women.</p>
</div>
<div id="chest-pain-distribution" class="section level2">
<h2>Chest pain distribution</h2>
<pre class="r"><code>heart %&gt;% 
  ggplot(aes(cp))+
  geom_histogram(fill = &quot;dodgerblue&quot;)+
  ggtitle(&quot;Chest pain&quot;, subtitle = &quot;chest pain type (1 = typical angina; 2 = atypical angina; 3 = non-anginal pain; 0 = asymptomatic)&quot;)</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div id="resting-blood-pressure" class="section level2">
<h2>Resting blood pressure</h2>
<pre class="r"><code>heart %&gt;% 
  ggplot(aes(trtbps))+
  geom_histogram(fill = &quot;dodgerblue&quot;)+
  ggtitle(&quot;Resting blood pressure&quot;, subtitle = &quot;resting blood pressure (in mm Hg on admission to the hospital)&quot;)+
  geom_errorbarh(aes(xmin = 90, xmax = 120, y = 20),height = 4)+
  annotate(&quot;text&quot;,label = &quot;NHS &#39;normal&#39; range&quot;, x = 105, y = 22)</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-10-1.png" width="672" />
There are a few observations with very high outlier resting blood pressure. According to the <a href="https://www.nhs.uk/common-health-questions/lifestyle/what-is-blood-pressure/">NHS</a>, a normal range for resting blood pressure in between 90 and 120.</p>
<pre class="r"><code>heart %&gt;% 
  filter(trtbps&gt;120) %&gt;% 
  count()</code></pre>
<pre><code>## # A tibble: 1 x 1
##       n
##   &lt;int&gt;
## 1   206</code></pre>
<p>The dataset skews heavily 67.99% outside of the ‘normal’ range for resting blood pressure.</p>
</div>
<div id="cholestoral-levels" class="section level2">
<h2>Cholestoral levels</h2>
<pre class="r"><code>heart %&gt;% 
  ggplot(aes(chol)) +
  geom_histogram(fill = &quot;dodgerblue&quot;) +
  ggtitle(&quot;Cholestoral levels&quot;,subtitle = &quot;serum cholestoral in mg/dl&quot;)+
  geom_errorbarh(aes(xmin = 125, xmax = 200, y = 20),height = 2) +
  annotate(&quot;text&quot;,label = &quot;&#39;normal&#39; range&quot;, x = 162.5, y = 22)</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-13-1.png" width="672" />
The normal range for cholesterol levels according to <a href="https://www.medicinenet.com/what_is_the_normal_range_for_cholesterol_levels/article.htm">medicinenet.com</a> is 125 to 200 mg/dL for men and women over age 20. Most of the observations fall outside of this range. There are very high outliers within the dataset with levels above 400 mg/dL.</p>
</div>
</div>
<div id="fitting-a-model-with-predictive-power" class="section level1">
<h1>Fitting a model with predictive power</h1>
<div id="splitting-the-dataset-into-training-and-testing-sets" class="section level3">
<h3>Splitting the dataset into training and testing sets</h3>
<p>This allows the model to be applied to previously unseen data to really test for accuracy.</p>
<pre class="r"><code>i &lt;- sample(c(rep(0, 0.7 * (nrow(heart)+1)),
              rep(1, 0.3 * (nrow(heart)+1))))


heart_train &lt;- heart[i == 0, ]       
heart_test &lt;- heart[i == 1, ]     



y &lt;- heart_train$output
#colnames(heart)[1:ncol(heart)-1]
x &lt;- data.matrix(heart_train[, c(colnames(heart)[1:ncol(heart)-1])])</code></pre>
</div>
<div id="fitting-the-lasso-regression" class="section level2">
<h2>Fitting the Lasso Regression</h2>
<p>The reason for choosing this type of model is because non-multicollinearity (predictor variables correlate with each other) is an assumption of an Ordinary Least Squares (OLS) regression. The lasso regression attempts to tackle this problem. The lasso regression includes a shrinkage penalty (<span class="math inline">\(\lambda\)</span>) to drop certain weakly significant variables from the model and aims to minimise over-fitting.</p>
<p><span class="math display">\[\sum\limits_{i = 1}^{N}(y_i - \hat{y} _i)^2 = \sum\limits_{i = 1}^{N}\left (y_i - \sum\limits_{j = 0}^{p}(\hat{\beta}_j \times x_{i, j})\right)^2 + \lambda \sum\limits_{j = 0}^{p}\mid{\hat{\beta}_j}\mid{}\]</span>
The MSE (mean-squared error) formula for the lasso regression above takes the form of a cost or optimisation function. The larger the coefficients, the greater the MSE is, and the greater the model is penalised. The model requires an optimisation of lambda <span class="math inline">\(\lambda\)</span> to shrink the absolute value of the coefficients <span class="math inline">\(\hat{\beta}_j\)</span> and optimise for the MSE. This penalty or shrinkage of the coefficients helps to reduce the models complexity and helps to reduce the impact of multi-collinearity between variables.</p>
<pre class="r"><code>library(glmnet)
heart_model &lt;- cv.glmnet(x, y, alpha = 1)

best_lambda &lt;- heart_model$lambda.min</code></pre>
<div id="minimising-mse-for-an-appropriate-lambda-and-coefficient-matrix" class="section level3">
<h3>Minimising MSE for an appropriate <span class="math inline">\(\lambda\)</span> and coefficient matrix</h3>
<pre class="r"><code>plot(heart_model)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<pre class="r"><code>best_model &lt;- glmnet(x, y, alpha = 1, lambda = best_lambda)
coef(best_model)</code></pre>
<pre><code>## 14 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                        s0
## (Intercept)  7.809654e-01
## age         -1.562930e-03
## sex         -1.469284e-01
## cp           1.118597e-01
## trtbps      -2.122455e-03
## chol        -9.974303e-05
## fbs          4.325654e-02
## restecg      4.015318e-02
## thalachh     2.469388e-03
## exng        -1.247495e-01
## oldpeak     -4.507179e-02
## slp          1.355036e-01
## caa         -1.285306e-01
## thall       -1.129364e-01</code></pre>
<p>There may be coefficients above that have been reduced to zero because of the nature of the Lasso regression model used. The Lasso regression is often implemented when there are lots of features and over-fitting a model may become a problem in producing an accurate model. Coefficients tending to zero are dropped from the model completely. There may be greater or fewer coefficients dropped depending on the sampling of the training dataset.</p>
</div>
</div>
<div id="measuring-the-r-squared-value-of-the-lasso-regression-model" class="section level2">
<h2>Measuring the R-Squared value of the lasso regression model</h2>
<pre class="r"><code>heart_test_x &lt;- data.matrix(heart_test[, c(colnames(heart)[1:ncol(heart)-1])])

heart_test_y &lt;- heart_test$output

predictions &lt;- predict(best_model, s = best_lambda, newx = heart_test_x)

sst &lt;- sum((y - mean(y))^2) # sum of squares total

sse &lt;- sum((predictions - heart_test_y)^2) # sum of squares error

rsq &lt;- 1 - sse/sst # r-squared

rsq</code></pre>
<pre><code>## [1] 0.7852579</code></pre>
<p>The R-squared turns out to be 0.7853. That is, the best model was able to explain 78.53% of the variation in the response values of the training data.</p>
</div>
<div id="confusion-matrix-how-correct-was-our-model" class="section level2">
<h2>Confusion matrix (how correct was our model?)</h2>
<pre class="r"><code>full_results &lt;- cbind(data.frame(heart_test_y),predictions, heart_test_x) %&gt;% 
  rename(&quot;prediction&quot; = &quot;1&quot;,
         &quot;output&quot; = &quot;heart_test_y&quot;)

confusion_matrix &lt;- full_results %&gt;% 
  mutate(correct_prediction = ifelse(output == round(prediction,0),&quot;CORRECT&quot;,&quot;INCORRECT&quot;),
         prediction_adj = round(prediction,0))


library(caret)</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## 
## Attaching package: &#39;caret&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     lift</code></pre>
<pre class="r"><code>library(InformationValue)</code></pre>
<pre><code>## Warning: package &#39;InformationValue&#39; was built under R version 4.0.5</code></pre>
<pre><code>## 
## Attaching package: &#39;InformationValue&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:caret&#39;:
## 
##     confusionMatrix, precision, sensitivity, specificity</code></pre>
<pre class="r"><code>optimal &lt;- optimalCutoff(actuals = confusion_matrix$output, predictedScores = confusion_matrix$prediction)

# The InformationValue::optimalCutoff function predicts a better rounding of probability. This may be more accurate than simply rounding to the nearest integer.  

optimal</code></pre>
<pre><code>## [1] 0.4516788</code></pre>
<pre class="r"><code>confusionMatrix(confusion_matrix$output, confusion_matrix$prediction)</code></pre>
<pre><code>##    0  1
## 0 36  4
## 1  9 42</code></pre>
<pre class="r"><code>(cf &lt;- confusion_matrix %&gt;% 
  count(correct_prediction))</code></pre>
<pre><code>##   correct_prediction  n
## 1            CORRECT 78
## 2          INCORRECT 13</code></pre>
<p>The lasso regression is correct for 85.71% of the testing observations.</p>
</div>
<div id="confusion-matrix-metrics" class="section level2">
<h2>Confusion matrix metrics</h2>
<pre class="r"><code>sensitivity(confusion_matrix$output, confusion_matrix$prediction)</code></pre>
<pre><code>## [1] 0.9130435</code></pre>
<pre class="r"><code>specificity(confusion_matrix$output, confusion_matrix$prediction)</code></pre>
<pre><code>## [1] 0.8</code></pre>
<pre class="r"><code>misClassError(confusion_matrix$output, confusion_matrix$prediction)</code></pre>
<pre><code>## [1] 0.1429</code></pre>
<p>The model has a misclassification rate of 14.29% which is quite reasonable for predicting if someone is at greater risk of heart disease.</p>
</div>
</div>
