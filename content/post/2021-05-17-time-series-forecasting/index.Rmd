---
title: Time series forecasting
author: ''
date: '2021-05-17'
slug: time-series-forecasting
categories: []
tags: []
---

```{r setup, include=FALSE, message=FALSE,warning=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE, message = FALSE)
```

```{r}
library(crypto)
library(RGENERATE)
library(quantmod)
library(lubridate)
library(tseries)
library(tidyverse)
library(forecast)

times <- RGENERATE::generate(FUN = rnorm, n = 1000, K = 3,names = c("asset1")) %>% 
  mutate(price = cumsum(asset1)+300) %>%
  mutate(day = row_number())

times %>% 
  ggplot(aes(day, price))+
  geom_line()+theme_minimal() +
  geom_smooth(method = 'lm', lty = 2, colour = 'red', se = F)+
  labs(title = "Dynamically created time series",
       subtitle = "Created using the RGENERATE pakage")

```

# ARIMA modelling assumptions

$$X_t = c + \epsilon_t + \sum_{i=1}^p\beta_iX_{t-i}+\sum_{i=1}^q\theta_i\epsilon_{t-i}$$

1. An ARIMA model is a regression combining an autoregression with integration and a moving average. 

2. For example, an ARIMA(3, 2, 5) model refers to a model with 3 autoregressive terms like $X_t = \beta_1X_{t-1}+\ldots+\beta_3X_{t-3}+\ldots$, 2 orders of differencing to remove non-stationarity, and 5 moving average terms $X_t=\ldots+\theta_{1} \epsilon_{t-1}+\ldots+\theta_5\epsilon_{t-5}$ 
3. We should be able to test to see which is the optimal number of terms to use for our model to aid in predictive power. 
4. The ARIMA model assumes that the data is univariate, that is it assumes there is only one explanatory variable, that being the previous consecutive price point. 
2. It also assumes the data to be stationary, that is the error term $\epsilon_t$ is normally distributed and considered to be 'white-noise'.

### Augmented Dickey-Fuller test
The Dickey-Fuller test checks to see if the time series is stationary. It is possible to simply plot the residuals of the data and then 'eyeball' it to see if the error terms largely tend towards zero and are normally distributed, that is $\epsilon_t$ ~ $\\N(0, \sigma^2)$. Otherwise...

$$\Delta y_t=c+\beta t + \gamma y_{t-1} + \sum_{i = 1}^{p}\delta\Delta y_{t-i} + e_t$$ where $c =$ regression constant. $\beta$, $\gamma$, $\delta_i$ are regression coefficients. $p =$ number of lagged terms. The Dickey-Fuller test tests for significance of the $\gamma$ coefficient with the null hypothesis $H_0 : \gamma = 1$, meaning that the lag coefficient is significant therefore the time series is non-stationary against $H_1:\gamma \neq1$, meaning that the lag variable coefficient is insignificant and that the model exhibits stationarity.

```{r}
(adf_output <- adf.test(times$price))

adf_output$p.value
adf_output$significant <- ifelse(adf_output$p.value<0.05,"significant","insignificant")


```

The p value of `r round(adf_output$p.value,2)` is `r adf_output$significant` which allows us to infer that our time series data exhibits `r ifelse(adf_output$significant == "significant","stationarity as there is enough evidence (p < 0.05) to suggest that there is no significant existance of a unit root. We are able to reject the null hypothesis that the coefficient is equal to 1","non-stationarity as the lag variable has a significant coefficient. We are unable to reject the null hypothesis that the coefficient is equal to 1")`.


In the case of non-stationarity, there are a few options to coerce stationarity such as taking the log or the square roots of the data to stabalise non-constant variance, calculating the residuals from another fitted model, or simply taking another difference of our series data, that is $\Delta^2$.

This can be demonstrated as follows

```{r}

times <- times %>% 
  mutate(diff_price = price - lag(price)) %>% 
  filter(!is.na(diff_price))

times %>% ggplot(aes(day, diff_price)) + geom_line()+
  geom_smooth(method = 'lm', lty = 2, colour = 'red', se = F)+
  theme_void()



ts(times$diff_price, start = c(2000,1), frequency = 365) %>% 
  decompose()%>% 
  plot()

adf.test(times$diff_price)

```

Many other unit root tests exist such as the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test. 


Alternatively, seasonality can be removed from the time series data using the in-built R stats functions.

```{r}
times.decomposed <- ts(times$price, start = c(2000,1), frequency = 365) %>% 
  decompose()
times.season.adj <- times$price - times.decomposed$seasonal

times.season.adj.test <- diff(times.season.adj, differences = 1)

plot(times.season.adj.test)
```


# Fitting the model

There are options to choose from for how we estimate the accuracy of our model. Namely, ML = Maximum Likelihood estimation, AIC = [Akaike Information Criterion](https://www.scribbr.com/statistics/akaike-information-criterion/),

```{r}
acf(times$price, lag.max=70)


fitARIMA <- arima(times$price, order=c(1,1,1),seasonal = list(order = c(1,0,0), period = 12),method="ML")


confint(fitARIMA)
```

Instead of going through each iteration of Arima(x, y, z) and selecting the best model, we can use the auto.arima() function in R from the forecasts package.

```{r}
times.arima_auto_model <- auto.arima(times$price %>% log(), trace = TRUE)
```


# Forecasting with the Arima model

```{r}
times.predictions <- forecast::forecast(object = times$price %>% log(), model = times.arima_auto_model, h = 30)




times.predictions %>% plot()


times.predictions <- as.tibble(times.predictions)



cbind(times.predictions, day = 1:nrow(times.predictions)+999) %>% 
  ggplot(aes(day, times.predictions$`Point Forecast`))+
  geom_ribbon(aes(ymin = `Lo 95`, ymax = `Hi 95`), alpha = 0.2, colour = "black", fill = "dodgerblue")+
  geom_ribbon(aes(ymin = `Lo 80`, ymax = `Hi 80`), alpha = 0.2, colour = "black", fill = "blue")+
  theme_minimal()+
  geom_line(lty = 2)+
  labs(title = "Forecasting asset price with the Arima model",
       subtitle = "A 60 day forecast - 95% and 80% confidence intervals",
       y = "price")
   

```



# Forecasting real-world price data

### Tesla

```{r}
#btc <- read_csv("btc.csv")

quantmod::getSymbols("TSLA")


tsla_model <- TSLA$TSLA.Close %>% auto.arima(trace = TRUE, stationary = FALSE)


tsla_forecast <- forecast::forecast(object = TSLA$TSLA.Close, model = tsla_model, h = 60)



forecast(tsla_model, h = 70) %>% plot()
```


### Amazon

```{r}
quantmod::getSymbols("AMZN")


AMZN_model <- log(AMZN$AMZN.Close) %>% auto.arima(trace = TRUE, stationary = FALSE)


AMZN_forecast <- forecast::forecast(object = AMZN$AMZN.Close, model = AMZN_model, h = 70)



forecast(AMZN_model, h = 70) %>% plot()


```

