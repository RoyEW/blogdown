---
title: Time series forecasting
author: ''
date: '2021-05-17'
slug: time-series-forecasting
categories: []
tags: []
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<pre class="r"><code>library(crypto)
library(RGENERATE)
library(quantmod)
library(lubridate)
library(tseries)
library(tidyverse)
library(forecast)

times &lt;- RGENERATE::generate(FUN = rnorm, n = 1000, K = 3,names = c(&quot;asset1&quot;)) %&gt;% 
  mutate(price = cumsum(asset1)+300) %&gt;%
  mutate(day = row_number())

times %&gt;% 
  ggplot(aes(day, price))+
  geom_line()+theme_minimal() +
  geom_smooth(method = &#39;lm&#39;, lty = 2, colour = &#39;red&#39;, se = F)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<div id="arima-modelling-assumptions" class="section level1">
<h1>ARIMA modelling assumptions</h1>
<p><span class="math display">\[X_t = c + \epsilon_t + \sum_{i=1}^p\beta_iX_{t-i}+\sum_{i=1}^q\theta_i\epsilon_{t-i}\]</span></p>
<ol style="list-style-type: decimal">
<li><p>An Arima model is a regression combining an autoregression with integration and a moving average.</p></li>
<li><p>For example, an ARIMA(3, 5) model refers to a model with 3 autoregressive terms like <span class="math inline">\(X_t = \beta_1X_{t-1}+\ldots+\beta_3X_{t-3}+\ldots\)</span>, and 5 moving average terms <span class="math inline">\(X_t=\ldots+\theta_{1} \epsilon_{t-1}+\ldots+\theta_5\epsilon_{t-5}\)</span></p></li>
<li><p>We should be able to test to see which is the optimal number of terms to use for our model to aid in predictive power.</p></li>
<li><p>The Arima model assumes that the data is univariate, that is it assumes there is only one explanatory variable, that being the previous consecutive price point.</p></li>
<li><p>Assumes the data is stationary, that is the error term <span class="math inline">\(\epsilon_t\)</span> is normally distributed and considered to be ‘white-noise’.</p></li>
</ol>
<div id="augmented-dickey-fuller-test" class="section level3">
<h3>Augmented Dickey-Fuller test</h3>
<p>The DF test checks to see if the times series is stationary. It is possible to simply plot the residuals of the data and then ‘eyeball’ it to see if the error terms largely tend towards zero and are normally distributed, that is <span class="math inline">\(\epsilon_t\)</span> ~ <span class="math inline">\(N(0, \sigma^2)\)</span>. Otherwise…</p>
<p><span class="math display">\[\Delta y_t=c+\beta t + \gamma y_{t-1} + \sum_{i = 1}^{p}\delta\Delta y_{t-i} + e_t\]</span> where <span class="math inline">\(c =\)</span> regression constant. <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\gamma\)</span>, <span class="math inline">\(\delta_i\)</span> are regression coefficients. <span class="math inline">\(p =\)</span> number of logs. The Dickey-Fuller tests for significance of the <span class="math inline">\(\gamma\)</span> coefficient with the null hypothesis <span class="math inline">\(H_0 : \gamma = 1\)</span>, the lag coefficient is significant therefore the times series is non-stationary against <span class="math inline">\(H_1:\gamma \neq1\)</span>, insignificant lag variable coefficient therefore the model exhibits stationarity.</p>
<pre class="r"><code>(adf_output &lt;- adf.test(times$price))</code></pre>
<pre><code>## 
##  Augmented Dickey-Fuller Test
## 
## data:  times$price
## Dickey-Fuller = -1.3044, Lag order = 9, p-value = 0.8728
## alternative hypothesis: stationary</code></pre>
<pre class="r"><code>adf_output$p.value</code></pre>
<pre><code>## [1] 0.87277</code></pre>
<pre class="r"><code>adf_output$significant &lt;- ifelse(adf_output$p.value&lt;0.05,&quot;significant&quot;,&quot;insignificant&quot;)</code></pre>
<p>The p value of 0.87 is insignificant which allows us to infer that our times series data exhibits non-stationarity as the lag variable has a significant coefficient. We are unable to reject the null hypothesis that the coefficient is equal to 1.</p>
<p>In the case of non-stationarity, there are a few options to coerce stationarity such as taking the log or the square roots of the data to stabalise non-constant variance, calculating the residuals from another fitted model, or simply taking another difference of our series data, that is <span class="math inline">\(\Delta^2\)</span>.</p>
<p>This can be demonstrated as follows</p>
<pre class="r"><code>times &lt;- times %&gt;% 
  mutate(diff_price = price - lag(price)) %&gt;% 
  filter(!is.na(diff_price))

times %&gt;% ggplot(aes(day, diff_price)) + geom_line()+
  geom_smooth(method = &#39;lm&#39;, lty = 2, colour = &#39;red&#39;, se = F)+
  theme_void()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>ts(times$diff_price, start = c(2000,1), frequency = 365) %&gt;% 
  decompose()%&gt;% 
  plot()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-3-2.png" width="672" /></p>
<pre class="r"><code>adf.test(times$diff_price)</code></pre>
<pre><code>## 
##  Augmented Dickey-Fuller Test
## 
## data:  times$diff_price
## Dickey-Fuller = -9.3357, Lag order = 9, p-value = 0.01
## alternative hypothesis: stationary</code></pre>
<p>Many other unit root tests exist such as the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test.</p>
<p>Alternatively, seasonality can be removed from the times series data using the in-built R stats functions.</p>
<pre class="r"><code>times.decomposed &lt;- ts(times$price, start = c(2000,1), frequency = 365) %&gt;% 
  decompose()
times.season.adj &lt;- times$price - times.decomposed$seasonal

times.season.adj.test &lt;- diff(times.season.adj, differences = 1)

plot(times.season.adj.test)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
</div>
<div id="fitting-the-model" class="section level1">
<h1>Fitting the model</h1>
<p>There are options to choose from for how we estimate the accuracy of our model. Namely, ML = Maximum Likelihood estimation, AIC = <a href="https://www.scribbr.com/statistics/akaike-information-criterion/">Akaike Information Criterion</a>,</p>
<pre class="r"><code>acf(times$price, lag.max=20)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>fitARIMA &lt;- arima(times$price, order=c(1,1,1),seasonal = list(order = c(1,0,0), period = 12),method=&quot;ML&quot;)


confint(fitARIMA)</code></pre>
<pre><code>##            2.5 %     97.5 %
## ar1  -2.12202783 2.10457732
## ma1  -2.11929206 2.10209371
## sar1 -0.09097231 0.03382827</code></pre>
<p>Instead of going through each iteration of Arima(x, y, z) and selecting the best model, we can use the auto.arima() function in R from the forecasts package.</p>
<pre class="r"><code>times.arima_auto_model &lt;- auto.arima(times$price, trace = TRUE)</code></pre>
<pre><code>## 
##  Fitting models using approximations to speed things up...
## 
##  ARIMA(2,1,2) with drift         : 2776.772
##  ARIMA(0,1,0) with drift         : 2777.354
##  ARIMA(1,1,0) with drift         : 2777.81
##  ARIMA(0,1,1) with drift         : 2778.7
##  ARIMA(0,1,0)                    : 2784.262
##  ARIMA(1,1,2) with drift         : 2781.792
##  ARIMA(2,1,1) with drift         : Inf
##  ARIMA(3,1,2) with drift         : 2781.457
##  ARIMA(2,1,3) with drift         : 2778.8
##  ARIMA(1,1,1) with drift         : 2779.805
##  ARIMA(1,1,3) with drift         : 2783.816
##  ARIMA(3,1,1) with drift         : 2782.48
##  ARIMA(3,1,3) with drift         : 2780.545
##  ARIMA(2,1,2)                    : 2783.709
## 
##  Now re-fitting the best model(s) without approximations...
## 
##  ARIMA(2,1,2) with drift         : 2778.975
## 
##  Best model: ARIMA(2,1,2) with drift</code></pre>
</div>
<div id="forecasting-with-the-arima-model" class="section level1">
<h1>Forecasting with the Arima model</h1>
<pre class="r"><code>times.predictions &lt;- forecast::forecast(object = times$price, model = times.arima_auto_model, h = 60)




times.predictions %&gt;% plot()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code>times.predictions &lt;- as.tibble(times.predictions)



cbind(times.predictions, day = 1:nrow(times.predictions)+999) %&gt;% 
  ggplot(aes(day, times.predictions$`Point Forecast`))+
  geom_ribbon(aes(ymin = `Lo 95`, ymax = `Hi 95`), alpha = 0.5, colour = &quot;black&quot;, fill = &quot;orange&quot;)+
  geom_ribbon(aes(ymin = `Lo 80`, ymax = `Hi 80`), alpha = 0.5, colour = &quot;black&quot;, fill = &quot;red&quot;)+
  theme_minimal()+
  geom_line(lty = 2)+
  labs(title = &quot;Forecasting asset price with the Arima model&quot;,
       subtitle = &quot;A 60 day forecast - 95% and 80% confidence intervals&quot;,
       y = &quot;price&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-7-2.png" width="672" /></p>
</div>
